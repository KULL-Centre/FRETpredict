{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDAnalysis\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import pandas as pd\n",
    "import mdtraj as md\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genPeaksAlexa594(dihedral_file):\n",
    "    frames = np.loadtxt(dihedral_file)\n",
    "    frames = np.deg2rad(frames[:, 1:10])\n",
    "    peaks = []\n",
    "\n",
    "    N = 360\n",
    "    width = (2*np.pi) / N\n",
    "    theta = np.linspace(-np.pi, np.pi, N, endpoint=True)\n",
    "    # Chi1\n",
    "    bins_x1 = np.histogram(frames[:, 0], bins=theta)\n",
    "    peak_11 = np.max(bins_x1[0][90:180])\n",
    "    peak_11 = np.where(bins_x1[0]==peak_11)\n",
    "    peak_12 = np.max(bins_x1[0][180:270])\n",
    "    peak_12 = np.where(bins_x1[0]==peak_12)\n",
    "    peak_13 = np.max(bins_x1[0][:90])\n",
    "    peak_13 = np.where(bins_x1[0]==peak_13)\n",
    "    peaks.append([int(peak_11[0]), int(peak_12[0]), int(peak_13[0][0])])\n",
    "\n",
    "    # Chi2\n",
    "    bins_x2 = np.histogram(frames[:, 1], bins=theta)\n",
    "    peak_21 = np.max(bins_x2[0][90:180])\n",
    "    peak_21 = np.where(bins_x2[0]==peak_21)\n",
    "    peak_22 = np.max(bins_x2[0][180:270])\n",
    "    peak_22 = np.where(bins_x2[0]==peak_22)\n",
    "    peak_23 = np.max(bins_x2[0])\n",
    "    peak_23 = np.where(bins_x2[0]==peak_23)\n",
    "    peaks.append([int(peak_21[0]), int(peak_22[0]), int(peak_23[0])])\n",
    "\n",
    "    # Chi3\n",
    "    bins_x3 = np.histogram(frames[:, 2], bins=theta)\n",
    "    peak_31 = np.max(bins_x3[0][90:180])\n",
    "    peak_31 = np.where(bins_x3[0]==peak_31)\n",
    "    peak_32 = np.max(bins_x3[0][180:270])\n",
    "    peak_32 = np.where(bins_x3[0]==peak_32)\n",
    "    peak_33 = np.max(bins_x3[0][270:])\n",
    "    peak_33 = np.where(bins_x3[0]==peak_33)\n",
    "    peaks.append([int(peak_31[0]), int(peak_32[0][0]), int(peak_33[0])])\n",
    "\n",
    "    # Chi4\n",
    "    bins_x4 = np.histogram(frames[:, 3], bins=theta)\n",
    "    peak_41 = np.max(bins_x4[0][0:90])\n",
    "    peak_41 = np.where(bins_x4[0]==peak_41)\n",
    "    peak_42 = np.max(bins_x4[0][90:180])\n",
    "    peak_42 = np.where(bins_x4[0]==peak_42)\n",
    "    peak_43 = np.max(bins_x4[0][180:270])\n",
    "    peak_43 = np.where(bins_x4[0]==peak_43)\n",
    "    peak_44 = np.max(bins_x4[0][270:])\n",
    "    peak_44 = np.where(bins_x4[0]==peak_44)\n",
    "    peaks.append([int(peak_41[0]), int(peak_42[0][0]), int(peak_43[0][2]), int(peak_44[0])])\n",
    "    \n",
    "    # Chi5\n",
    "    bins_x5 = np.histogram(frames[:, 4], bins=theta)\n",
    "    peak_51 = np.max(bins_x5[0][90:180])\n",
    "    peak_51 = np.where(bins_x5[0]==peak_51)\n",
    "    peak_52 = np.max(bins_x5[0][180:270])\n",
    "    peak_52 = np.where(bins_x5[0]==peak_52)\n",
    "    peak_53 = np.max(bins_x5[0])\n",
    "    peak_53 = np.where(bins_x5[0]==peak_53)\n",
    "    peaks.append([int(peak_51[0]), int(peak_52[0]), int(peak_53[0])])\n",
    "\n",
    "    # Chi6\n",
    "    bins_x6 = np.histogram(frames[:, 5], bins=theta)\n",
    "    peak_61 = np.max(bins_x6[0][90:180])\n",
    "    peak_61 = np.where(bins_x6[0]==peak_61)\n",
    "    peak_62 = np.max(bins_x6[0][180:270])\n",
    "    peak_62 = np.where(bins_x6[0]==peak_62)\n",
    "    peak_63 = np.max(bins_x6[0])\n",
    "    peak_63 = np.where(bins_x6[0]==peak_63)\n",
    "    peaks.append([int(peak_61[0][0]), int(peak_62[0]), int(peak_63[0])])\n",
    "\n",
    "    # Chi7\n",
    "    bins_x7 = np.histogram(frames[:, 6], bins=theta)\n",
    "    peak_71 = np.max(bins_x7[0][90:180])\n",
    "    peak_71 = np.where(bins_x7[0]==peak_71)\n",
    "    peak_72 = np.max(bins_x7[0][180:270])\n",
    "    peak_72 = np.where(bins_x7[0]==peak_72)\n",
    "    peak_73 = np.max(bins_x7[0])\n",
    "    peak_73 = np.where(bins_x7[0]==peak_73)\n",
    "    peaks.append([int(peak_71[0]), int(peak_72[0]), int(peak_73[0])])\n",
    "    \n",
    "    # Chi8\n",
    "    bins_x8 = np.histogram(frames[:, 7], bins=theta)\n",
    "    peak_81 = np.max(bins_x8[0][90:180])\n",
    "    peak_81 = np.where(bins_x8[0]==peak_81)\n",
    "    peak_82 = np.max(bins_x8[0][180:270])\n",
    "    peak_82 = np.where(bins_x8[0]==peak_82)\n",
    "    peak_83 = np.max(bins_x8[0])\n",
    "    peak_83 = np.where(bins_x8[0]==peak_83)\n",
    "    peaks.append([int(peak_81[0]), int(peak_82[0]), int(peak_83[0])])\n",
    "\n",
    "    # Chi9\n",
    "    bins_x9 = np.histogram(frames[:, 8], bins=theta)\n",
    "    peak_91 = np.max(bins_x9[0][:180])\n",
    "    peak_91 = np.where(bins_x9[0]==peak_91)\n",
    "    peak_92 = np.max(bins_x9[0][180:])\n",
    "    peak_92 = np.where(bins_x9[0]==peak_92)\n",
    "    peaks.append([int(peak_91[0]), int(peak_92[0])])\n",
    "\n",
    "    return frames, np.array(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genPeaksAlexa488(dihedral_file):\n",
    "    frames = np.loadtxt(dihedral_file)\n",
    "    frames = np.deg2rad(frames[:, 1:10])\n",
    "    peaks = []\n",
    "\n",
    "    N = 360\n",
    "    width = (2*np.pi) / N\n",
    "    theta = np.linspace(-np.pi, np.pi, N, endpoint=True)\n",
    "    # Chi1\n",
    "    bins_x1 = np.histogram(frames[:, 0], bins=theta)\n",
    "    peak_11 = np.max(bins_x1[0][90:180])\n",
    "    peak_11 = np.where(bins_x1[0]==peak_11)\n",
    "    peak_12 = np.max(bins_x1[0][180:270])\n",
    "    peak_12 = np.where(bins_x1[0]==peak_12)\n",
    "    peak_13 = np.max(bins_x1[0][:90])\n",
    "    peak_13 = np.where(bins_x1[0]==peak_13)\n",
    "    peaks.append([int(peak_11[0]), int(peak_12[0]), int(peak_13[0][0])])\n",
    "\n",
    "    # Chi2\n",
    "    bins_x2 = np.histogram(frames[:, 1], bins=theta)\n",
    "    peak_21 = np.max(bins_x2[0][90:180])\n",
    "    peak_21 = np.where(bins_x2[0]==peak_21)\n",
    "    peak_22 = np.max(bins_x2[0][180:270])\n",
    "    peak_22 = np.where(bins_x2[0]==peak_22)\n",
    "    peak_23 = np.max(bins_x2[0])\n",
    "    peak_23 = np.where(bins_x2[0]==peak_23)\n",
    "    peaks.append([int(peak_21[0]), int(peak_22[0]), int(peak_23[0])])\n",
    "\n",
    "    # Chi3\n",
    "    bins_x3 = np.histogram(frames[:, 2], bins=theta)\n",
    "    peak_31 = np.max(bins_x3[0][90:180])\n",
    "    peak_31 = np.where(bins_x3[0]==peak_31)\n",
    "    peak_32 = np.max(bins_x3[0][180:270])\n",
    "    peak_32 = np.where(bins_x3[0]==peak_32)\n",
    "    peak_33 = np.max(bins_x3[0][270:])\n",
    "    peak_33 = np.where(bins_x3[0]==peak_33)\n",
    "    peaks.append([int(peak_31[0]), int(peak_32[0]), int(peak_33[0])])\n",
    "\n",
    "    # Chi4\n",
    "    bins_x4 = np.histogram(frames[:, 3], bins=theta)\n",
    "    peak_41 = np.max(bins_x4[0][0:90])\n",
    "    peak_41 = np.where(bins_x4[0]==peak_41)\n",
    "    peak_42 = np.max(bins_x4[0][90:180])\n",
    "    peak_42 = np.where(bins_x4[0]==peak_42)\n",
    "    peak_43 = np.max(bins_x4[0][180:270])\n",
    "    peak_43 = np.where(bins_x4[0]==peak_43)\n",
    "    peak_44 = np.max(bins_x4[0][270:])\n",
    "    peak_44 = np.where(bins_x4[0]==peak_44)\n",
    "    peaks.append([int(peak_41[0]), int(peak_42[0][1]), int(peak_43[0]), int(peak_44[0])])\n",
    "    \n",
    "    # Chi5\n",
    "    bins_x5 = np.histogram(frames[:, 4], bins=theta)\n",
    "    peak_51 = np.max(bins_x5[0][90:180])\n",
    "    peak_51 = np.where(bins_x5[0]==peak_51)\n",
    "    peak_52 = np.max(bins_x5[0][180:270])\n",
    "    peak_52 = np.where(bins_x5[0]==peak_52)\n",
    "    peak_53 = np.max(bins_x5[0])\n",
    "    peak_53 = np.where(bins_x5[0]==peak_53)\n",
    "    peaks.append([int(peak_51[0]), int(peak_52[0]), int(peak_53[0])])\n",
    "\n",
    "    # Chi6\n",
    "    bins_x6 = np.histogram(frames[:, 5], bins=theta)\n",
    "    peak_61 = np.max(bins_x6[0][90:180])\n",
    "    peak_61 = np.where(bins_x6[0]==peak_61)\n",
    "    peak_62 = np.max(bins_x6[0][180:270])\n",
    "    peak_62 = np.where(bins_x6[0]==peak_62)\n",
    "    peak_63 = np.max(bins_x6[0])\n",
    "    peak_63 = np.where(bins_x6[0]==peak_63)\n",
    "    peaks.append([int(peak_61[0][0]), int(peak_62[0]), int(peak_63[0])])\n",
    "\n",
    "    # Chi7\n",
    "    bins_x7 = np.histogram(frames[:, 6], bins=theta)\n",
    "    peak_71 = np.max(bins_x7[0][90:180])\n",
    "    peak_71 = np.where(bins_x7[0]==peak_71)\n",
    "    peak_72 = np.max(bins_x7[0][180:270])\n",
    "    peak_72 = np.where(bins_x7[0]==peak_72)\n",
    "    peak_73 = np.max(bins_x7[0])\n",
    "    peak_73 = np.where(bins_x7[0]==peak_73)\n",
    "    peaks.append([int(peak_71[0]), int(peak_72[0]), int(peak_73[0])])\n",
    "    \n",
    "    # Chi8\n",
    "    bins_x8 = np.histogram(frames[:, 7], bins=theta)\n",
    "    peak_81 = np.max(bins_x8[0][90:180])\n",
    "    peak_81 = np.where(bins_x8[0]==peak_81)\n",
    "    peak_82 = np.max(bins_x8[0][180:270])\n",
    "    peak_82 = np.where(bins_x8[0]==peak_82)\n",
    "    peak_83 = np.max(bins_x8[0])\n",
    "    peak_83 = np.where(bins_x8[0]==peak_83)\n",
    "    peaks.append([int(peak_81[0]), int(peak_82[0]), int(peak_83[0])])\n",
    "\n",
    "    # Chi9\n",
    "    bins_x9 = np.histogram(frames[:, 8], bins=theta)\n",
    "    peak_91 = np.max(bins_x9[0][:180])\n",
    "    peak_91 = np.where(bins_x9[0]==peak_91)\n",
    "    peak_92 = np.max(bins_x9[0][180:])\n",
    "    peak_92 = np.where(bins_x9[0]==peak_92)\n",
    "    peaks.append([int(peak_91[0]), int(peak_92[0])])\n",
    "\n",
    "    return frames, np.array(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genClusters(prefix='genLIB',dye='alexa488'):\n",
    "    if dye=='alexa488':\n",
    "        frames, original_peaks = genPeaksAlexa488(prefix+'/{:s}_scan.dat'.format(dye))\n",
    "    if dye=='alexa594':\n",
    "        frames, original_peaks = genPeaksAlexa594(prefix+'/{:s}_scan.dat'.format(dye))\n",
    "    peaks = np.array(list(itertools.product(*original_peaks))) - 180\n",
    "    cluster_dict = {}\n",
    "    final_cluster_dict = {}\n",
    "    frames = np.rad2deg(frames)\n",
    "    for frame_index, i in enumerate(frames):\n",
    "        least_column = (i - peaks) ** 2\n",
    "        least_column1 = ((i + 360) - peaks) ** 2\n",
    "        least_column2 = ((i - 360) - peaks) ** 2\n",
    "        indexing1 = least_column1 < least_column\n",
    "        indexing2 = least_column2 < least_column\n",
    "        if np.sum(indexing1)>0:\n",
    "            least_column[indexing1] = least_column1[indexing1]\n",
    "        if np.sum(indexing2)>0:\n",
    "            least_column[indexing2] = least_column2[indexing2]\n",
    "        least = np.sum(least_column, axis=1)\n",
    "        min_index = np.argmin(least)\n",
    "        if min_index not in cluster_dict.keys():\n",
    "            cluster_dict[min_index] = [i]\n",
    "        else:\n",
    "            cluster_dict[min_index].append(i)\n",
    "    new_peaks = np.zeros((len(cluster_dict.keys()),9))\n",
    "    for index, value in enumerate(cluster_dict.keys()):\n",
    "        new_peaks[index] = np.mean(cluster_dict[value], axis=0)\n",
    "    for frame_index, i in enumerate(frames):\n",
    "        least_column = (i - new_peaks) ** 2\n",
    "        least_column1 = ((i + 360) - new_peaks) ** 2\n",
    "        least_column2 = ((i - 360) - new_peaks) ** 2\n",
    "        indexing1 = least_column1 < least_column\n",
    "        indexing2 = least_column2 < least_column\n",
    "        if np.sum(indexing1)>0:\n",
    "            least_column[indexing1] = least_column1[indexing1]\n",
    "        if np.sum(indexing2)>0:\n",
    "            least_column[indexing2] = least_column2[indexing2]\n",
    "        least = np.sum(least_column, axis=1)\n",
    "        min_index = np.argmin(least)\n",
    "        min_value = least[min_index]\n",
    "        if min_index not in final_cluster_dict.keys():\n",
    "            final_cluster_dict[min_index] = [frame_index, min_value, 1]\n",
    "        else:\n",
    "            if min_value < final_cluster_dict[min_index][1]:\n",
    "                final_cluster_dict[min_index][0] = frame_index\n",
    "                final_cluster_dict[min_index][1] = min_value\n",
    "                final_cluster_dict[min_index][2] += 1\n",
    "            else:\n",
    "                final_cluster_dict[min_index][2] += 1\n",
    "    pickle_file = open(prefix+'/clusters_{:s}_1step.p'.format(dye), 'wb')\n",
    "    fileout = pickle.dump(final_cluster_dict, pickle_file)\n",
    "    pickle_file.close()\n",
    "    np.savetxt(prefix+'/{:s}_avg_peaks.dat'.format(dye),new_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterCluster(cutoff,prefix='genLIB',dye='alexa488'):\n",
    "    pickle_file = open(prefix+'/clusters_{:s}_1step.p'.format(dye), 'rb')\n",
    "    cluster_dict = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "    discarded_frames = []\n",
    "    discarded_peaks = []  \n",
    "    frequency = []\n",
    "    print('Initial clusters',len(cluster_dict.keys())) \n",
    "    for cluster in cluster_dict.keys():\n",
    "        if cluster_dict[cluster][2] < cutoff:\n",
    "            discarded_frames.append(cluster_dict[cluster][0])\n",
    "            discarded_peaks.append(cluster)\n",
    "            frequency.append(cluster_dict[cluster][2])\n",
    "            \n",
    "    for key in discarded_peaks:\n",
    "        cluster_dict.pop(key)\n",
    "    print('Final clusters',len(cluster_dict.keys())) \n",
    "\n",
    "    frames = np.loadtxt(prefix+'/{:s}_scan.dat'.format(dye))[:, 1:10]\n",
    "\n",
    "    sel_peaks = pd.read_csv(prefix+'/{:s}_avg_peaks.dat'.format(dye),sep=' ',header=None)\n",
    "    sel_peaks = sel_peaks.loc[list(cluster_dict.keys())]\n",
    "    \n",
    "    for frame_index, i in enumerate(frames[discarded_frames]):\n",
    "        least_column = (i - sel_peaks) ** 2\n",
    "        least_column1 = ((i + 360) - sel_peaks) ** 2\n",
    "        least_column2 = ((i - 360) - sel_peaks) ** 2\n",
    "        indexing1 = least_column1.values < least_column.values\n",
    "        indexing2 = least_column2.values < least_column.values\n",
    "        if np.sum(indexing1)>0:\n",
    "            least_column.values[indexing1] = least_column1.values[indexing1]\n",
    "        if np.sum(indexing2)>0:\n",
    "            least_column.values[indexing2] = least_column2.values[indexing2]\n",
    "        least = least_column.sum(axis=1)\n",
    "        cluster_dict[least.idxmin()][2] += frequency[frame_index]\n",
    "    pickle_file = open(prefix+'/clusters_{:s}_{:d}cutoff.p'.format(dye,cutoff), 'wb')\n",
    "    fileout = pickle.dump(cluster_dict, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genRotLib(cutoff,conf,traj,prefix='genLIB',dye='alexa488'):\n",
    "    pickle_file = open(prefix+'/clusters_{:s}_{:d}cutoff.p'.format(dye,cutoff), 'rb')\n",
    "    clusters = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    selected_frames = []\n",
    "    frequency = []\n",
    "    for cluster in clusters.keys():\n",
    "        frequency.append(clusters[cluster][2])\n",
    "        selected_frames.append(clusters[cluster][0])\n",
    "    print(np.sum(frequency))\n",
    "    u = MDAnalysis.Universe(prefix+'/'+conf, prefix+'/'+traj)\n",
    "    alexa = u.select_atoms('resname S{:s}'.format(dye[-3:]))\n",
    "    Ca_pos = u.select_atoms('resname S{:s} and name CA'.format(dye[-3:]))\n",
    "    N_pos = u.select_atoms('resname S{:s} and name N'.format(dye[-3:]))\n",
    "    C_pos = u.select_atoms('resname S{:s} and name C'.format(dye[-3:]))\n",
    "    new_coords = np.empty(0)\n",
    "    for ts in u.trajectory[selected_frames]:\n",
    "        offset = Ca_pos.positions.copy()\n",
    "        Ca_cords = Ca_pos.positions - offset\n",
    "        N_cords = N_pos.positions - offset\n",
    "        C_cords = C_pos.positions - offset\n",
    "        alexa_coords = alexa.positions - offset\n",
    "        x_vector = N_cords - Ca_cords\n",
    "        x_vector /= np.linalg.norm(x_vector)\n",
    "        yt_vector = C_cords - Ca_cords\n",
    "        yt_vector /= np.linalg.norm(yt_vector)\n",
    "        z_vector = np.cross(x_vector, yt_vector)\n",
    "        z_vector /= np.linalg.norm(z_vector)\n",
    "        y_vector = np.cross(z_vector, x_vector)\n",
    "        rotation = np.array((x_vector, y_vector, z_vector)).T\n",
    "        alexa_coords = np.dot(alexa_coords, rotation.reshape(3,3))\n",
    "        alexa_coords = alexa_coords.reshape((len(alexa_coords), 3))\n",
    "        new_coords = np.append(new_coords, alexa_coords)\n",
    "    new_coords = new_coords.reshape((len(selected_frames), len(alexa), 3))\n",
    "    output_file = open(prefix+'/rot_lib_matrix_{:s}_{:d}cutoff.txt'.format(dye,cutoff), 'w')\n",
    "    for index, conformer in enumerate(new_coords):\n",
    "        for atom_index, atom in enumerate(conformer):\n",
    "            output_file.write('{0:>3} {1:>3} {2[0]:> 10.6f} {2[1]:> 10.6f} {2[2]:> 10.6f} {3:>5}\\n'.format(index+1, \n",
    "                                                                    alexa[atom_index].id, atom, frequency[index]))\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='genLIB' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial clusters 1338\n",
      "Final clusters 407\n",
      "100001\n",
      "Initial clusters 1219\n",
      "Final clusters 360\n",
      "100001\n"
     ]
    }
   ],
   "source": [
    "for dye in ['alexa488','alexa594']: \n",
    "    #genClusters(prefix,dye)\n",
    "    for cutoff in [75]:\n",
    "        filterCluster(cutoff,prefix,dye)\n",
    "        traj = 'c_s{:s}_ff99sbws_remd_npt_s1_nd0_whole.xtc'.format(dye[-3:])\n",
    "        conf = 'c_s{:s}_ff99sbws_vac.pdb'.format(dye[-3:])\n",
    "        genRotLib(cutoff,conf,traj,prefix,dye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407.0 100001.0\n",
      "317.0 100001.0\n",
      "205.0 100001.0\n",
      "134.0 100001.0\n",
      "360.0 100001.0\n",
      "288.0 100001.0\n",
      "181.0 100001.0\n",
      "125.0 100001.0\n"
     ]
    }
   ],
   "source": [
    "for dye in ['alexa488','alexa594']: \n",
    "    for cutoff in [75,100,150,200]:\n",
    "        conf = 'c_s{:s}_ff99sbws_vac.pdb'.format(dye[-3:])\n",
    "        u = MDAnalysis.Universe(prefix+'/'+conf)\n",
    "        alexa = u.select_atoms('resname S{:s}'.format(dye[-3:]))\n",
    "        a = np.loadtxt('/Users/giulio/FRETpredict/FRETpredict/lib/rot_lib_matrix_{:s}_{:d}cutoff.txt'.format(dye,cutoff), dtype='float32',\n",
    "                       usecols=(5))\n",
    "        print(a.shape[0]/len(alexa),a.sum()/len(alexa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.0 100001.0\n",
      "59.0 100001.0\n",
      "25.0 100001.0\n",
      "10.0 100001.0\n",
      "82.0 100001.0\n",
      "32.0 100001.0\n",
      "16.0 100001.0\n",
      "11.0 100001.0\n"
     ]
    }
   ],
   "source": [
    "for dye in ['alexa488','alexa594']: \n",
    "    for cutoff in [75,100,150,200]:\n",
    "        conf = 'c_s{:s}_ff99sbws_vac.pdb'.format(dye[-3:])\n",
    "        u = MDAnalysis.Universe(prefix+'/'+conf)\n",
    "        alexa = u.select_atoms('resname S{:s}'.format(dye[-3:]))\n",
    "        a = np.loadtxt('/Users/giulio/FRETpredict/FRETpredict/lib/rot_lib_matrix_{:s}_3step_{:d}cutoff.txt'.format(dye,cutoff), dtype='float32',\n",
    "                       usecols=(5))\n",
    "        print(a.shape[0]/len(alexa),a.sum()/len(alexa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
